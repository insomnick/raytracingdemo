%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Vorlage für das Dokument laden:
%   - scrartl  -> Artikel
%   - scrreprt -> Bericht
%   - scrbook  -> Buch
\documentclass[
    a4paper,
    11pt,
    headings=small    
]{scrreprt}

% Eingabecodierung utf8
\usepackage[utf8]{inputenc}
% Ausgabecodierung von Sonderzeichen
\usepackage[T1]{fontenc}
% Silbentrennung und Sprachanpassungen
\usepackage[ngerman]{babel}

% AMS-Pakete für mathematische Formeln
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[centerdots]{mathtools}

% Weitere nützliche Pakete
\usepackage{xcolor}   % farbiger Text
\usepackage{graphicx} % Für das Einbinden von Bildern
\usepackage{csquotes} % Für korrekte Zitate
\usepackage{hyperref} % Für Hyperlinks
\usepackage{geometry} % Für Seitenränder
\usepackage{scrlayer-scrpage} % Für Kopf- und Fußzeilen
\usepackage{setspace} % Für Zeilenabstand
\usepackage{listings} % für Programmcode/Pseudocode
\usepackage{lmodern}  % schoenere Schriftart
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{caption}
\usepackage[backend=biber, style=alphabetic]{biblatex} % Für die Einbindung von BibTeX

\input{template-config.tex}

% Einstellungen für Seitenlayout
\geometry{a4paper, left=3cm, right=3cm, top=2.5cm, bottom=3.5cm}

% Einstellungen für Kopf- und Fußzeilen
\pagestyle{scrheadings}
\clearpairofpagestyles
\chead{\leftmark}
\cfoot[\pagemark]{\pagemark}
\automark[section]{section}

% BibTeX-Datei einbinden
\addbibresource{literatur.bib}

% Titel und Autor
\subject{Bachelorarbeit}
\title{Analyse der Qualität von Top-Down erzeugten Wide BVHs in statischen 3D-Szenen} % Titel der Arbeit eintragen
\author{Nick Garczorz}        % Namen eintragen
\publishers{%
\textsf{Heinrich-Heine-Universität Düsseldorf}\\[\baselineskip]
\begin{tabular}{rl}
Erstgutachter:in: & Dr. Andreas Abels\\ % Namen eintragen
Zweitgutachter:in:& Dr. Markus Brenneis\\ % Namen eintragen
\end{tabular}}
\date{\today}

% Listings für Codeschnipsel
\lstdefinelanguage{Pseudocode}{
  morekeywords={function,return,if,then,else,end,while,do,for,each,continue},
  sensitive=true,
  morecomment=[l]{//},
  morestring=[b]"
}

\lstdefinestyle{codestyle}{
  language=Pseudocode,
  basicstyle=\ttfamily\scriptsize,
  keywordstyle=\bfseries,
  commentstyle=\itshape\color{gray},
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=6pt,
  tabsize=2,
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  columns=fullflexible,
  frame=single,
  captionpos=b,
  xleftmargin=1em,
  framexleftmargin=0.5em,
  keepspaces=true,
}

\begin{document}

\maketitle

\chapter*{Zusammenfassung}
Bounding Volume Hierarchies (BVHs) gehören zu den wichtigsten Beschleunigungsdatenstrukturen zur effizienten Berechnung von Ray-Szenen-Schnittpunkten in der Bildprojektion. Während Wide BVHs (Verzweigungsgrad $k > 2$) bereits im Zusammenhang mit Optimierung durch GPU-Berechnung untersucht werden, ist es weniger klar, ob sie auch bei serieller Berechnung auf der CPU einen praktischen Vorteil durch ihre Struktur bieten können.

Diese Arbeit untersucht Top-Down konstruierte BVH in statischen dreidimensionalen Szenen für Verzweigungsgrade $k \in {2, 4 ,8 ,16}$. Dies geschieht unter Berücksichtigung von drei Splitting-Verfahren (Median, Surface Area Heuristic und Binned Surface Area Heuristic), sowie zwei Methoden zur Erzeugung von Wide BVHs (k-way splitting und binary BVH collapse). Dabei steht die Traversal-Time als primäres und die Construction-Time als sekundäres Qualitätsmerkmal im Fokus. Zudem wird durch eine Kombination der beiden Qualitätsmetriken ein Szenario mit dynamischer Szene modelliert.

Für alle untersuchten statischen Szenen zeigt $k = 4$ eine statistisch signifikante Verbesserung gegenüber der vergleichbaren binären Variante. Dabei liegt der Traversal-Speedup-Faktor im Mittel zwischen 1,03 und 1,08, wobei der Effekt für größere Szenen abzuflachen scheint. Für höhere Verzweigungsgrade ($k = 8$ und $k = 16$) liegt der Speedup-faktor deutlich und statistisch signifikant unter 1. 
Es konnte zwischen k-way splitting und der collapse Methode keine signifikanten Unterschiede in der Traversal-Time gefunden werden.
Auch im dynamischen Modell scheint der durch $k = 4$ entstehende Vorteil mit steigender Szenengröße abzunehmen und kann bei den größeren untersuchten Szenen, unabhängig vom Konstruktionsverfahren, sogar ausbleiben bzw. zum Nachteil werden. 
Insgesamt deuten die Ergebisse darauf hin, dass bei serieller Berechnung auf der CPU ein Verzweigungsgrad von $k = 4$ einen kleinen aber klaren Gewinn liefert. größere Verzweigungsgrade sind nicht zu Empfehlen. Das Verhalten in größeren Szenen bietet Raum für zukünftige Forschung.


\tableofcontents

\newpage

\chapter{Einleitung}

\section{Motivation}

Effiziente Kollisionserkennung hat grundlegenden Nutzen in diversen Feldern, über physikbasierte Simulation und Robotik, bis hin zu Darstellungen in Animation und Videospielen~\cite{ericson2004real}. Im Fall von Animation und Bildgebung im Allgemeinen handelt es sich vor allem um Kollisionen einfacher geometrischer Formen, häufig Geraden oder Strahle, Dreiecke und Quader. Unter Zuhilfenahme dieser einfachen Werkzeuge lassen sich komplexe dreidimensionale Szenen mit einfachen Mitteln berechnen und darstellen. Für jeden Bildpunkt werden Strahlen, genannt Rays, auf die digitale Szene geworfen. Diese besteht in gängiger Praxis aus einer Ansammlung von Dreiecken, genannt Polygone, oder anderen fundamentalen Formen, die allgemein als Primitive bezeichnet werden. Um herauszufinden, ob und wo es eine Kollision zwischen einem Ray und einem bestimmten Polygon gibt, kann man Beispielsweise den weitverbreiteten Algorithmus von Möller und Trumbore~\cite{Möller01011997} verwenden. Wenn es einen Treffer gibt, lässt sich aufbauend auf Position und Abstand des Schnittpunktes der korrekte Farbwert für den Pixel berechnen. Diese Rechnung müsste man naiv gesehen für jede Kombination aus Rays und Primitive wiederholen, um ein vollständiges Bild zu generieren. 

\section{Thema und fachliche Einordnung}

Um den Vorgang zu beschleunigen, kommen zwei Datenstrukturen ins Spiel: Die Axis Aligned Bounding Box (AABB) und darauf aufbauend die Bounding Volume Hierarchy (BVH)~\cite{VahlGordonClevenger+2021}. Eine AABB definiert achsenorientierte obere und untere Schranken für Teilmengen von Primitives in Form eines Quaders, der diese im Raum einschließt. Durch die Einführung von AABB lassen sich die nötigen Ray-Object-Intersection-Tests verringern, indem man für jeden Ray erst die Kollision mit den AABBs überprüft. Wenn es bereits keine Kollision mit der umschließenden Bounding Box gibt, kann von vornherein ausgeschlossen werden, dass es eine Kollision mit den innenliegenden Primitiven gibt. 
Um den Vorteil dieses Verfahrens effektiv zu nutzen, kann man nun rekursiv die eingeschlossenen Teilmengen wieder aufteilen und mit AABBs umschließen. Dadurch lassen sich die Bounding Boxes in eine hierarchische Reihenfolge bringen, die eine Baumstruktur aufweist. Diese Struktur ist eine Bounding Volume Hierarchy. Durch eine BVH wird die Laufzeit auf ein Niveau gesenkt, das typischerweise bei geeigneter Ray-Verteilung logarithmisch von der Anzahl an Primitives in der Szene abhängt, was die Zeit für ein fertig gerendertes Bild drastisch sinken lassen kann. Diesen Vorteil erkauft man sich durch das Einführen des zusätzlichen Konstruktionsschrittes, der in Abwägung dazu steht. 
Die sinnvolle Konstruktion einer BVH ist ein Kernpunkt dieser Arbeit. 



\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{images/bunny_boxes.png}
\caption{Stanford Bunny mit sichtbaren Bounding Boxes}
\end{figure}

\section{Zielsetzung}

Es existieren Konstruktionsverfahren verschiedener Komplexitätsklassen, deren Endprodukt sich auf die Traversal Time (Renderzeit) auswirkt. Von der Construction Time (Konstruktionszeit) lässt sich die Nutzbarkeit einer BVH in dynamischen Szenen, also Szenen mit bewegten Objekten, ableiten. Die Traversal Time ist das Hauptqualitätsmerkmal für eine BVH, da sich durch niedrigere Zeiten höhere Auflösungen und Bildwiederholraten, sowohl in statischen wie auch dynamischen Szenen, realisieren lassen.
Ein genutzter Vorteil der Baumstruktur ist, dass sich sowohl Construction (Konstruktion) als auch Traversal (Durchsuchung) auf GPUs parallelisieren lassen. Dabei spielt auch die konstruktion mit höheren Verzweigungsgraden eine Rolle, wobei man auch dies durch unterschiedliche Konstruktionsverfahren erreichen kann, wie dem \emph{k-way splitting} oder durch \emph{collapse} (Kollabierung) der Hierarchie. Diese Art von BVH wird Wide BVH (Weite BVH) oder auch BVHk genannt. 
Ziel der Arbeit ist die Analyse verschiedener Konstruktionsverfahren von Wide BVHs mit Hinblick auf Einsparung in Traversal- aber auch Construction-Zeit in statischen Szenen. Diese Art von BVH ist bereits in der parallelisierten Berechnung mit GPUs weiter verbreitet, hier wird allerdings eine serielle Berechnung auf der CPU untersucht und damit die algorithmischen Vor-und Nachteile von Wide BVH. Dabei sollen die Ergebnisse abhängig von dem verwendeten konstruktionsalgorithmus, dem Verzweigungsgrad der Hierarchie und der Primitive-Anzahl der Szene beleuchtete werden. 
Dabei haben sich drei Problemfragen ergeben, die den Scope dieser Arbeit definieren:
\begin{itemize} 
\item
Wie verändert sich die Traversal-Time bei Top-Down-Konstruierten Wide BVHs für Verzweigungsgrade $k \in \{2, 4, 8, 16\}$ in statischen Szenen?
\item
Unterscheidet sich die Traversal-Time der BVHs zwischen direktem k-way splitting und collapse einer binären BVH abhängig von der gewählten statischen Szene?
\item
Wie verändern sich Construction- und Traversal-Time als kombinierte Qualitätsmetrik für die modellierung dynamischer Szenen mit Bezug auf das gewählte Splitting-Verfahren jeweils über $k \in \{2, 4, 8, 16\}$?
\end{itemize}

Die Analyse soll dabei auf einer selbstimplmentierten Testbench stattfinden und anschließend mit unabhängigen T-Tests ausgewertet werden. Mittelwerte, sowie Konfidenzintervalle dienen zur späteren Diskussion der Ergebnisse.

\section{Abgrenzung zu vorherigen Arbeiten}
In der zugrundeliegenden Arbeit werden lediglich statische dreidimensionale Szenen untersucht. Vefahren die darauf abzielen die BVH dynamisch anzupassen (Refitting etc.) werden nicht untersucht.

Zudem werden ausschließlich Top-Down-Konstruktionsmethoden verwendet, da diese in Forschung und Lehre weit verbreitet und einfacher zu implementieren sind~\cite{https://doi.org/10.1111/cgf.142662}.

Die Verglichenen Datenstrukturen sind lediglich BVH bzw Wide BVH, es werden keine anderen Beschleunigungsstrukturen, wie k-d-Trees beachtet. Wide BVHs werden im Vergleich zu ihrer binären Referenzimplementation untersucht. 

Es werden ganz explizit verschiedene Verfahren zur Konstruktion von Wide BVHs untersucht und verglichen. Das sogenannte k-way splitting, also das Partitionieren der Menge in k Teilmengen in einem Iterationsschritt entlang einer Achse, ist das erste Untersuchte Verfahren. Auf der anderen Seite steht das Collapse-Verfahren bei dem Nodes einer Binären Hierarchie nach der Konstruktion mit ihren Kindern verschmolzen werden um den Verzweigungsgrad zu verdoppeln.

Es werden lediglich Median-Split und SAH-Split, sowie die Optimierungsvariante binned SAH-Split untersucht, keine komplexeren Varianten wie Spatial-Splits. Zudem ist die Anzahl der Bins auf 16 festgelegt. Der Fokus soll nicht auf den verwendet Algorithmen, sondern auf den Auswirkungen der Verfahren bezüglich dem Verzweigungsgrad liegen.

Die gemessenen Metriken beziehen sich auf CPU-Zeit, also lineare Berechnungen. Spezielle Raytracing-Hardware oder parallelisierte Messergebnisse werden nicht untersucht, das der Fokus auf Gewinn durch strukturelle Eigenschaften der Hierarchien liegen soll.

Die bei der Traversal-Time verwendeten Rays sind lediglich \emph{Primärstrahlen}, also direkte Strahlen zwischen virtueller Kamera und Objekt. Dadurch fallen Verfahren wie Reflektion, Refraktion o.Ä. aus der Bewertung, was die Vergleichbarkeit vereinfacht und die Komplexität verringert.   

Die Qualität einer BVH wird rein durch die Traversal-Time definiert. Andere Aspekte wie zum Beispiel Bildqualität durch Verwendung verschiedener Shader sind nicht Teil der Untersuchung.

Die Besonderheit dieser Arbeit ist, dass Wide BVH komplett außerhalb des Kontext der Parallelisierabrkeit analysiert werden und alle Berechnungen seriell auf der CPU ausgeführt werden. Es wird explizit die die gesamte serielle Zeit der Algorithmen untersucht, um Veränderungen durch Struktur der BVHs zu ermitteln. 

\newpage

\chapter{Grundlagen}

\section{Modell}

Für das weitere vorgehen werden unter Primitives immer Polygone, also Dreiecke, verstanden. Polygone werden durch ihre drei Eckpunkte definiert, die jeweils durch dreidimensionale Vektoren beschrieben werden. zudem wird für die Vereinfachung von Berechnungen der Mittelpunkt des Polygons bestimmt, \emph{Centroid} genannt. Eine zusammenhängende Struktur aus Polygonen wird als \emph{Mesh} verstanden. 

Wenn sich im Traversal-Schritt die Frage stellt, ob ein Ray ein Primitive schneidet, stellt sich eigentlich die Frage, ob ein bestimmtes Primitive das erste ist, was der untersuchte Ray schneidet. Man spricht hier von \emph{closest hit test}~\cite{https://doi.org/10.1111/cgf.142662}.

\section{Axis Aligned Bounding Box}

Um Konstruktion und Traversal von Nodes einer BVH möglichst einfach zu gestalten müssen einige Punkte gegeben sein: 
\begin{itemize}
    \item Sie müssen eine möglichst performante Möglichkeit bieten die Kollision mit einem Ray zu überprüfen.
    \item Ihre räumlichen Grenzen sollen möglichst einfach Berechnet werden können.
    \item Sie sollen möglichst wenig Speicherplatz verbrauchen, bzw. möglichst wenig Zusatzinformationen benötigen.
\end{itemize}
Die am häufigsten verwendete Datenstruktur, die diese Punkte miteinander vereint ist die Axis Aligned Bounding Box. Man kann sie sich vorstellen wie ein an den Koordinatenachsen ausgerichteter Quader, der die darin enthaltenen Primitives möglichst Eng einschließt. Die räumlichen Grenzen lassen sich durch zwei Vektoren, die die oberen und unteren Schranken der Box definieren leicht definieren und durch eine lineare Suche in den enthaltenen Primitives relativ effizient berechnen.

Die Kollision mit Rays ist durch die \emph{SLAB-Methode} sehr performant~\cite{10.1145/15922.15916}. Hierbei ...yxc

\section{Bounding Volume Hierarchy}

Eine BVH ist im Kern eine Baumstruktur, die eine sinnvolle Partiotionierung der Primitive in der Szene erwirkt. Um einen inneren Knoten zu definieren benötigt man eine AABB, Referenzen zu den eingeschlossenen Primitives und Referenzen zu ihren Child-Nodes. Die AABBs von Child-Nodes befinden sich immer innerhalb der grenzen der AABB ihres Parent-Nodes. Es ist allerdings möglich, dass die AABBS der Child-Nodes sich überschneiden. Die Wurzel ist gleich definiert. Die Blätter der Datenstruktur sind konzeptionel Nodes, deren Menge an Primitives kleiner gleich dem festgelegten Verzweigungsgrad sind. 
Qualitätsmerkmale einer BVH sind in etwa wenig Überschneidungen von AABBs, AABBs mit möglichst kleinen Flächeninhalten und möglichst wenig Kollisionsüberprüfungen mit Primitives~\cite{10.1145/2492045.2492056}. 

Dabei hängt die Qualität einer BVH von der verwendeten Konstruktionsmethode ab, welche wiederum variierende Komplexität haben. Häufig existiert eine Abwägung zwischen Construction und Traversal Time. 

\subsection{Wide Bounding Volume Hierarchy}

Wide BVH werden für das Arbeiten mit Grafikkarten immer interessanter, da sich durch die erweiterte Breite der BVH noch mehr Möglichkeiten für parallelisiertes Arbeiten ergeben. Dabei existiert für die in dieser Arbeit untersuchten Algorithmen immer eine Referenzmethode zur Erstellung von Binärbäumen. Die untersuchten Verzweigungsgrade sind vielfache von Zwei. Dadurch wird die Vergleichbarkeit zwischen den verschiedenen Konstruktionsmethoden gewährleistet, da das Collapse-Verfahren in dieser Implementation auf Binärbäumen basiert.
Die direkte Konsequenzen von Wide BVH sind eine geringere Tiefe und mehr Kollisionschecks mit Child-Nodes pro Knoten. Vor- und Nachteile bezogen auf Construction und Traversal werden innerhalb dieser Arbeit untersucht.

\subsection{Traversal}
Der Traversal-Schritt ist der Kerngrund für die Verwendung einer BVH. Die Traversal-Time zu minimieren ist ihre Hauptauptaufgabe. Die Traversal geschieht dabei Top-Down durch die Hierarchie. Falls es eine Kollision zwischen Ray und AABB eines Knotens gibt, sollen auch die Child-Nodes, bzw. bei einem Blatt die Primitives, überprüft werden. Falls nicht kann der gesamte Teilbaum ausgelassen werden. Die Reihenfolge wird hier durch einen Stack gegeben. In diesem Fall handelt es sich um eine Art der Breitensuche mit frühem Abbruchkriterium pro Teilbaum.
Die Zeit für einen AABB-Test und einen Primitive-Test unterscheiden sich, wodurch sich ein weiteres Abbruchkriterium ergeben kann. Dies geschieht indem man die Zeit für Ray-Primitive-Kollision, die Dauer des durchsuchens eines Teilbaums und die Menge an Primitiven im Teilbaum in eine Kostengleichung einträgt. Es existieren auch Vereinfachungen dieser Gleichung, diese werden im folgenden Erklärt.

\subsection{Kostenmodelle}
Es gibt eine Reihe von sogenannten \emph{Traversal-Cost-Models}~\cite{meister2022performance}. Hierbei gibt es eine \emph{Cost- bzw. Loss-Function} die bei der Konstruktion des Baumes verwendet wird um den, für die Funktion, optimalen Split zu finden.

Das Grundgerüst der Kostenfunktion sieht in den meisten Fällen ähnlich aus und unterscheidet sich je nach Heuristik in der bemessenen Wahrscheinlichkeit, mit der der Teilbaum eines gegebene Nodes besucht wird.  
Dabei entsprechen die Kosten eines inneren Nodes $N$:
\[
c(N) = c_T + \sum_{N_{child}} P(N_{child} \mid N)\ c(N_{child})
\]

Dabei entspricht $c_T$ der durchschnittlichen Zeit für einen Schritt während der Traversal und $N_{child}$ wird als Iterator über alle Kinder des Knoten $N$ verstanden.

Falls es sich nicht um einen inneren Node handelt ist die Kostenfunktion $c(N) = c_I \lvert N \rvert$ mit Ray-Primitive-Intersection-Time $c_I$ und Anzahl an Primitiven in Node $\lvert N \rvert$.

Das bekannteste Kostenmodell zur Konstruktion einer BVH ist die \emph{Surface Area Heuristic}, welche die in dieser Arbeit untersuchte Heuristik darstellt. Es existieren allerdings eine ganze Reihe an Modellen wie zum Beispiel die \emph{Ray Distribution Heuristic (RDH)}, \emph{Occlusion Heuristic (OH)} oder \emph{End-point Overlap Heuristic (EPO)}~\cite{https://doi.org/10.1111/cgf.142662}, um eine Auswahl zu nennen, deren Untersuchung nicht Teil dieser Arbeit ist.

\subsection{Construction}

Die Konstruktion von BVH wird klassischerweise entweder Top-Down oder Bottom-Up durchgeführt. Der Großteil der verwendeten Algorithmen sind Top-Down-Construction Algorithmen, welche exklusiv in dieser Arbeit besprochen werden.


\subsubsection{Median Split}

Die naivste Partitionsstrategie ist der Median Split. Dabei wird die entlang einer Achse sortierte Menge an Primitiven am Median geteilt. Das wird rekursiv für die neu entstandenen Teilmengen wiederholt, bis ein festgelegtes Abbruchkriterium erreicht wird. Sie basiert damit nicht auf einm Kostenmodell und eignet sich vor allem wegen ihrer einfachheit für den Vergleich. Sie besitzt de facto den kleinsten Overhead für die Konstruktion, da keine Abwägung der Teilmengen nötig ist. Die Idee ist, dass dadurch strukturelle Änderungen der BVH klarer herausstechen.

\subsubsection{Surface Area Heuristic Split}

Dagegen stehen Traversal-Kostenmodelle wie die Surface Area Heuristic (SAH). Von ihr leiten sich einige spezialisierte Partitionierungsstrategien ab, wovon eine auch näher in dieser Arbeit besprochen wird.

Die Grundidee ist es, die Wahrscheinlichkeit, dass ein Ray auf die AABB eines Nodes trifft, als geometrische Wahrscheinlichkeit zu sehen. Das bedeutet, dass der Flächeninhalt der AABB einer Node mit den potenziellen Flächeninhalten ihrer Child-Nodes ins Verhältnis gesetzt werden~\cite{meister2022performance}. Daraus ergibt sich, dass kleinere Flächeninhalte die Kostenfunktion minimieren, also als besser gesehen werden. Die SAH berücksichtigt dabei nicht die tatsächliche Ausrichtung und verteilung von Rays.

\[
P(N_{child} \mid  N)_{SAH} = \frac{Area(N_{child})}{ Area(N)} 
\]

Um die Kostenfunktion zu minimieren muss jeder mögliche Split untersucht werden. Da das Kostenmodell verwendet wird um Vergleichswerte zu erstellen und nicht absolute Werte zu errechnen, können Konsatnten, wie die Traversal-Time des Teilbaums oder die Zeit für die Kollisionserkennung vernachlässigt werden, was die Funktion stark vereinfacht.

Theoretisch lassen sich durch die Verwendung dieser Konstanten und das Berechnen absoluter Werte ein neues Abbruchkriterium für das Erstellen einer BVH ermitteln, allerding müssten diese Konstanten im besten Fall vorher empirisch bestimmt werden. Da ein optimales Kostenmodell nicht der Schwerpunkt dieser Arbeit ist, wurde dies nicht behandelt. 

\subsubsection{Binned Surface Area Heuristic}

Die Binned Surface Area Heuristic ist eine Abwandlung, der SAH. Sie verwendet das gleiche Kostenmodell, hat allerdings einen Implementationsspezifischen Unterschied. Statt Splits zwischen jedem Primitive zuzulassen, werden die Primitives vorerst entlang der Achse in eine beliebige Menge Bins, im diesem falle 16, einsortiert. Dann wird der optimale Split zwischen diesen Bins ermittelt. Der Vorteil ist, dass lediglich ein Bruchteil an Berechnungen gemacht werden muss, um einen Split zu berechnen. Dieser Gewinn in der Construction-Time geht allerdings gegebenenfalls auf Kosten der BVH-Qualität.

\newpage

\chapter{Methoden}

\section{Aufbau}

\subsection{Überblick}

Verglichen werden BVH in verschieden großen Szenen, die sich in Konstruktionsalgorithmus und Verzweigungsgrad unterscheiden. Die Untersuchten Algorithmen sind der \emph{Median Split, Surface Area Heurustic Split} und \emph{Binned Surface Are Heuristic Split}. Als Baseline wird in jedem Fall ein Binärer Testdurchlauf mit Verzweigungsgrad $k=2$ durchgeführt. Zudem werden $k=4$, $k=8$ und $k=16$ untersucht. Dabei existiert jeweils eine Variante in der die binäre BVH collapsed wird um die höheren Grade zu erreichen und eine Variante mit direktem k-way-split. 
Innerhalb der Szenen wird eine einfache Kamerafahrt in 36 Frames unterteilt. Bei der Kamerafahrt handelt es sich um eine einfachen Pfad um den Ursprung der Szene, bei dem dieser stehts im Mittelpunkt steht. In den meisten Szenen rotiert damit die Kamera um ein Objekt. Die Frames unterscheiden sich damit lediglich in Kameraposition und -Ausrichtung. 
Dabei werden als Hauptmetriken die Construction-Time und die Traversal-Time erfasst. 
Insgesamt werden pro Testdurchlauf 10 Datenpunkte für die Construction-Time und dann pro Frame jeweils die Traversal-Time gesammelt. 
Alle Tests wurden um Messfehler auszuschließen 10 mal wiederholt.
Damit kommt man für jede Permutation von Algorithmus, Verzweigungsgrad, Wide BVH Variante und Modell auf 84 verschiedene Testruns mit je 46 Datenpunkte und 10 Wiederholungen. Also gesamtheitlich auf \emph{38.640 Datenpunkte}.
Im ersten Schritt werden Unterschiede in der Traversal-Time untersucht um auf Veränderungen in der BVH-Qualität zurückzuschließen. Im zweiten Schritt wird die Summe von Construction- und Traversal-Time betrachtet, um die Performance in einer dynamischen Szene zu modellieren, da in dynamischen Szenen die BVH theoretisch nach jeder Änderung an der Szene neu berechnet werden muss.  

Die gewählten Szenen sind bekannte Testmodelle mit unterschiedlichen Größenordnungen im Bezug auf Polygonanzahl, die sich in einem ansonsten leeren Raum aufhalten.
Die Szenengröße kann man folgender Tabelle entnehmen:

\begin{table}[h]
    \centering
    \begin{tabular}{l l r}
    \hline
    Szene & Polygonanzahl \\
    \hline
    Suzanne & 968 \\
    Utah Teapot & 6320 \\
    Stanford Bunny & 69451 \\
    Stanford Armadillo & 99976 \\
    \hline
    \end{tabular}
    \caption{Polygonanzahl untersuchter 3D-Modelle}
    \label{tab:object-meta}
    \end{table}
    

\begin{figure}[h]
    \centering
    \includegraphics[width=0.24\textwidth]{images/suzanne.png}
    \includegraphics[width=0.24\textwidth]{images/teapot.png}
    \includegraphics[width=0.24\textwidth]{images/bunny.png}
    \includegraphics[width=0.24\textwidth]{images/armadillo.png}
    \caption{Suzanne, Utah Teapot, Stanford Bunny. Stanford Armadillo}
\end{figure}

Zu jedem Frame wird wie oben dargestellt ein Bild erzeugt, welches dazu dient die Korrektheit der Traversal zu untersuchen. Damit kann zudem sicehrgestellt werden, dass die untersuchte BVH eine gültige Form besitzt. Unabhängig von von Testrun sollen die erzeugten Bilder pro Testszene immer gleich sein. 

\subsection{Datenmodell}

Punkte im Raum werden als Vektoren im Stil von 3er-Tupeln erfasst, hier \emph{Vector3} genannt.

Die implementierte BVH verwendet nicht direkt Polygone, sondern ist in der Lage Primitive verschiedener Arten zu verarbeiten, solange sie die benötigten Grundeigenschaften besitzen. Dazu gehören ein Mittelpunkt (Center oder auch Centroid), einen Minimal- und Maximalpunkt im Sinne einer Bounding Box, die nur das Primitive selbst einschließt und eine Funktion um Kollisionen (Intersections) mit Rays zu erkennen und gegebenenfalls den dazugehörigen Punkt im Raum zu bestimmen. 
Polygone verfügen über all diese Eigenschaften.

\begin{lstlisting}[style=codestyle,caption={Primitive}]
class Primitive 
{
    Vector3 getCenter()
    Vector3 getMin()
    Vector3 getMax()
    Vector3 intersect(ray)
}
\end{lstlisting}

AABBs verfügen über ähnliche Attribute. Sie benötigen allerdings keine referenz zu ihrem Mittelpunkt, da sie während der Laufzeit nicht entlang einer Achse sortiert werden müssen. 

\begin{lstlisting}[style=codestyle,caption={Axis Alignet Bounding Box}]
class AABB 
{
    Vector3 getMin()
    Vector3 getMax()
    Vector3 intersect(ray)
}
\end{lstlisting}

Die BVH selbst besteht aus aufeinander referenzierende Nodes. Ein Node wird dabei durch seine Bounding Box, Referenzen zu den eingeschlossenen Primitiven und Referenzen zu seinen Child-Nodes definiert.

Innerhalb der BVH muss so nur eine Referenz auf die Wurzel (Root) gegeben sein, alle anderen Referenzen erfolgen implizit. 
Zudem liegt die Working-Copy aller Primitives innerhalb der BVH, auf die die jeweiligen Nodes jeweils ihre referenzen besitzen. 
\begin{lstlisting}[style=codestyle,caption={Bounding Volume Hierarchie}]
class BVHNode 
{
    AABB aabb
    Primitve [] primitives
    BVHNode [] children 
}

class BVH
{
    BVHNode root
    Primitive [] primitives
}
\end{lstlisting}

Das festgelegte Leaf-Kriterium ist wie folgt: Es existieren gleich oder weniger Primitive in der zu teilenden Menge, als vom Verzweigungsgrad verlangte splits nötig sind. 

\section{Algorithmenanalyse}

\subsection{Traversal}

Für die Traversal wird ein Stack verwendet. Anfangs wird die Root der BVH auf den Stack gelegt. Im folgenden wird immer der oberste Node vom Stack genommen. 

Falls es eine Kollision zwischen Ray und Bounding Box des Nodes gibt, werden falls vorhanden alle Child-Nodes auf den Stack gelegt, andernfalls geschieht nichts. Für denn Fall, dass es sich um einen Leaf-Node handelt, werden die Primitive des Nodes näher angeschaut. 

Kommt es zu einer Kollision zwischen Ray und Primitive, wird die Distanz zwischen Ray-Origin und Schnittpunkt berechnet. Ist nun die Distanz kürzer als die jeder zuvor gefundene Kollision, wird sie gespeichert. Für komplexere Raytracer würde man den Schnittpunkt, sowie Eigenschaften des Primitives, wie Material oder Oberflächenwinkel des Rays, abzuspeichern, dies ist in diesem Anwendungsfall allerdings nicht notwendig. 

Die Funktion gibt letztendlich die kürzeste Distanz einer Kollision zurück. Diese Art von Traversal wird als \emph{closest hit} bezeichnet.

\begin{lstlisting}[style=codestyle,caption={BVH Traversal},mathescape]
function traverse(bvh, ray):
    stack := empty list
    push(stack, bvh.root)
    
    closestDistance := +$\infty$

    while stack is not empty do
        node := pop(stack)
    
        if not node.aabb.intersect(ray) then
            continue
        end if

        if node.children is empty then  //Leaf node
            for primitive in node.primitives do
                intersection := primitive.intersect(ray)
                if intersection then
                    distance := distance(ray.origin, intersection)
                    if distance < closestDistance then
                        closestDistande := distance
                    end if
                end if
            end for
        end if
        
        for child in node.children do
            push(stack, child)
        end for
    end while
    return closestDistance

end function

\end{lstlisting}

\subsection{Construction}

Die Konstruktionsfunktion unterstützt eine variable Split-Funktion, hier \emph{partitionFunction} mit variablem Verzweigungsgrad.
Nachdem eine Kopie fer Primitive für die BVH erstellt und sichergegangen wird, dass überhaupt Primitive in der Szene existieren, wird die \emph{Root-Node} erstellt. Diese wird auf einen Stack gelegt. 

Solange Nodes auf dem Stack liegen, wird der oberste vom Stack genommen und behandelt. Nachdem die längste Achse der korrespondierenden AABB berechnet wurde, wird die ausgewählte Split-Funtion mit den Primitiven des Nodes und der Achse aufgerufen. Die daraus resultierenden Indexe werden verwendet um die Primitive aufzuteilen. Falls keine Indexe zurückgegeben werden, wird der behandelte Node als Leaf angesehen und der Schleifendurchlauf abgebrochen. Für alle neuen Teilmengen werden dann neue AABBs berechnet und neue Nodes erstellt. Diese neue Nodes werden in den behandelten Nodes als Child-Nodes referenziert. Daraufhin werden sie auf den Stack gelegt und die Schleife beginnt von neuen. 

Letztendlich wird eine BVH-Instanz mit dem zuerst berechneten Root-Node und der Kopie aller Primitive zurückgegeben. 

\begin{lstlisting}[style=codestyle,caption={BVH-Konstruktion mit gegebener Partitionsfunktion}]
function build(inputPrimitives, partitionFunction):
    // move input primitives into a single owned list inside the BVH
    ownedPrimitives := copy of inputPrimitives

    if ownedPrimitives is empty then
        return BVH(root = emptyRoot, primitives = ownedPrimitives)
    end if

    // compute global bounds of all primitives
    box := computeBoundingBox(ownedPrimitives)

    // create BVH with a single root node
    rootNode := BVHNode(box       = box,
                       primitives = ownedPrimitives
                       children   = empty list)
    bvh      := BVH(root = rootNode, primitives = ownedPrimitives)

    // stack of nodes to process
    nodes := empty list
    push(nodes, bvh.root)

    while nodes is not empty do
        node := pop(nodes)

        // choose splitting axis based on longest extent
        axis := calculateLongestAxis(node.box)

        // Find right subsets according to partitionFunction
        splitIndices      := partitionFunction(primitives, axis)
        if splitIndices is empty then
            continue //Leaf Node
        end if        
        primitiveSubsets  := calculateSubsets(primitives, splitIndices)

        for each primitiveSubset in primitiveSubsets do
            boundingBox  := computeBoundingBox(primitiveSubset)
            child        :=  BVHNode(box       = boundingBox,
                                    primitives = primitiveSubset
                                    children   = empty list)
            append child to node.children
        end for

        // push children onto stack for further splitting
        for each child in node.children do
            push(nodes, reference to child)
        end for
    end while

    return bvh
end function
\end{lstlisting}

\subsubsection{Median Split}

Der Median-Split ist die einfachste variante einer Split-Funtion. Die Menge der Primitive wird in, vom Verzweigungsgrad abhängige, gleichgroße Teile gespalten. Dafür wird die Menge der Primitive $n$ durch den Verzweigungsgrad geteilt um die Größe einer Teilmenge zu bestimmen. Vielfache dieser Zahl innerhalb des Intervalls $(0, n)$ werden als Indexe fürs Splitting verwendet. Allerdings muss dafür noch die Menge an Primitiven, entlang der gegebenen Achse, partiell Sortiert werden, sodass für alle Indexe gilt, dass der Mittelpunkt der Position der Primitive links des Index kleiner sind als die rechts des Indexes.   

\begin{lstlisting}[style=codestyle,caption={Median Partition}]
function medianSplit(primitives, axis, degree):    //O(n)
    count := primitives.size()
    splitIndices := empty list

    for i in degree do
        split := (count * i) / degree
        if split == 0 or split >= count then
            break
        end if
        push(splitIndices, split)
    end for

    // partially sort so that element at split is in its median position in O(n)
    for split in splitIndices do 
        nth_element(set    = primitives,
                    middle = split,
                    compare(primitiveA, primitiveB):
                        return center(primitiveA).component(axis) < center(primitiveB).component(axis))
    end for
    return splitIndices
end function
\end{lstlisting}

\subsubsection{Surface Area Heuristic Split}

\begin{lstlisting}[style=codestyle,caption={Surface Area Heuristic Partition},mathescape]
function sahSplit(primitives, axis, degree):   //O(n * log(n))

    count := primitives.size()
    if isLeaf(count, degree) then
        return {}
    end if
    
    //O(n * log(n))
    sort(set = primitives,
         compare(primitiveA, primitiveB):
             return center(primitiveA).component(axis) < center(primitiveB).component(axis))

    splitIndices := empty list
    segments := empty list
    push(segments, primitives)
    while segments.size() < degree do

        seg := findSegmentToSplitGreedily(segments)   //highest cost segment has to be split
        // bounds of primitives from segment from begin to index
        prefixBoundingBoxes := calculatePrefix(seg)  //O(n)
        // bounds of primitives from segment from index to end (included)
        suffixBoundingBoxes := calculateSuffix(seg) // O(n)

        segCount := seg.count() 
        split   := segCount / 2 // default
        minCost := +$\infty$
        for i from 1 to segCount - 2 do
            leftArea  := surfaceArea(prefixBoundingBoxes[i - 1])
            rightArea := surfaceArea(suffixBoundingBoxes[i])
            cost      := leftArea * i + rightArea * (segCount - i) //simplified cost function
            if cost < minCost then
                minCost := cost
                split   := i
            end if
        end for
        
        push(splitIndices, seg.begin + split)   //convert relative split to absolute
        erase(segments, seg)
        //push subsets
        push(segments, seg[seg.begin, split])
        push(segments, seg[split, seg.end])
    end do
    return splitIndices
end function
\end{lstlisting}

\subsubsection{Binned Surface Area Heuristic Split}


\begin{lstlisting}[style=codestyle,caption={Binned Surface Area Heuristic Partition},mathescape]
    function binnedSahSplit(primitives, axis, degree):   //O(n * log(n))
    
        Bin_SIZE := 16  // b

        count := primitives.size()
        if isLeaf(count, degree) then
            return {}
        end if
        
        //O(n * log(n))
        sort(set = primitives,
             compare(primitiveA, primitiveB):
                 return center(primitiveA).component(axis) < center(primitiveB).component(axis))
    
        struct Bin {count; bounds}        
        
        bins := sortPrimitivesIntoBins(primitives, BIN_SIZE) //O(n)

        splitIndices := empty list
        segments := empty list
        push(segments, bins) // [0, BIN_SIZE)
        while segments.size() < degree do
    
            seg := findSegmentToSplitGreedily(segments)   //highest cost segment has to be split
            // bounds of primitives from segment from begin to index
            prefixBoundingBoxes := calculatePrefix(seg)  //O(b)
            // bounds of primitives from segment from index to end (included)
            suffixBoundingBoxes := calculateSuffix(seg) // O(b)

            
            segCount := seg.count() 
            split   := segCount / 2 // default
            minCost := +$\infty$
            for i from 1 to segCount - 2 do
                leftArea  := surfaceArea(prefixBoundingBoxes[i - 1])
                rightArea := surfaceArea(suffixBoundingBoxes[i])
                cost      := leftArea * i + rightArea * (segCount - i) //simplified cost function
                if cost < minCost then
                    minCost := cost
                    split   := i
                end if
            end for
            
            absoluteSplit := calculateAbsoluteSplit(bins, split)
            push(splitIndices, absoluteSplit)
            erase(segments, seg)
            //push subsets
            push(segments, seg[seg.begin, split])
            push(segments, seg[split, seg.end])

        end do
        return splitIndices
    end function
    \end{lstlisting}

\subsection{Collapse Funktion}

\begin{lstlisting}[style=codestyle,caption={BVH-Collapsing}]
    function collapse(bvh):

        // stack of nodes to process
        nodes := empty list
        push(nodes, bvh.root)

        while nodes is not empty do
            node := pop(nodes)

            if node.children is empty do
                continue
            end if

            new_children := empty list

            for each child in node.children do
                if child.children is empty do
                    push(new_children, child)
                    continue
                end if
                push(new_children, child.children)
            end do

            node.children := new_children
            push(nodes, node.children)
        end while
        
        return bvh
    end function
    \end{lstlisting}


\newpage

\chapter{Ergebnisse}

\section{Korrektheit}
Um zu sichern, dass der BVH-Vergleich nicht durch falsche bzw. ungleiche Ausgaben verfälscht wird, wurde über alle Konfigurationen hinweg ein aus Primärstrahlen berechnetes Bild erzeugt. Die dabei entstandene Bilder sind identisch. Für jede Szene und Kameraposition wurden die generierten Bilder mit denen der anderen Konfigurationen Bitweise verglichen, um eine gleiche Ausgabe zu garantieren. 

Außerdem wurde die Anzahl an \emph{Hitrays} pro Frame erfasst. Ein Hitray beschreibt dabei einen Primärstrahl, der minimum ein Primitiv in der Szene geschnitten hat. Auch die Anzahl an Hitrays ist über alle Konfigurationen pro Szene und Frame identisch, womit sichergestellt werden kann, dass über alle Konfigurationen auch die gleiche Menge an Traversalschritten erfolgreich war.  

\section{Traversal}
Die erste Forschungsfrage beschäftigt sich damit, wie sich die Traversal-Time Top-Down konstruierter Wide BVHs abhängig vom Verzweigungsgrad $k \in \{2, 4, 8, 16\}$ verändert. 
Hierfür dient als Referenz für jede Szene und jedes Splitting-Verfahren eine binäre BVH ($k = 2$). Zum Zweck der Vergleichbarkeit wird das Feld \emph{Speedup} definiert: 
\[
Speedup = \frac{T_{\mathrm{\mu, traversal}(k = 2)}}{T_{\mu, \mathrm{traversal}(k)}}
\]
Dabei bezeichnet $T_{\mathrm{\mu, traversal}(k)}$ den Mittelwert der, wie im Kapitel Methoden gezeigt, aggregierten gemessenen Traversal-Times, abhängig vom Verzweigungsgrad $k$. 
Werte $Speedup > 1$ gelten als Verbesserung, also geringerer Traversal-Time, gegenüber der binären Referenz. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.2]{results/static_combined_speedup_k4.png}
    \includegraphics[scale=0.2]{results/static_combined_speedup_k8.png}
    \includegraphics[scale=0.2]{results/static_combined_speedup_k16.png}
    \caption{Abbildung Traversal-Speedup für statische Szenen}
    \label{fig:static}
\end{figure}

Die Abbildung~\ref{fig:static} zeigt den Traversal-Speedup für $k \in {4, 8, 16}$ und Tabelle~\ref{tab:static} enthält die dazugehörigen Mittelwerte, Konfidenzintervalle und p-Werte für jede kombination von Szene, Splittingverfahren und Wide BVH Methode.
Für $k = 4$ gibt es über alle Kombinationen eine konsistente und statistisch signifikante Verbesserung gegenüber der binären Referenz. Die Mittelwerte der Speedups liegen zwischen ungefähr $1,03$ und $1,08$.
Für $k = 8$ verschlechtert sich die Traversal-Time in allen untersuchten Fällen klar mit Speedup-Mittelwerten zwischen ungefähr $0,63$ und $0,77$.
Für $k = 8$ fällt die Performance nochmal deutlich ab mit Mittelwerten zwischen \ $0,10$ und $0,19$, was einer stark höheren Traversal-Time gegenüber $k = 2$ entspricht.

\section{Wide BVH Konstruktionsmethode}
Die zweiten Forschungsfrage bezieht sich darauf, inwiefern sich die Traversal-Time zwischen den beiden Untersuchten Verfahren zur Erzeugung einer Wide BVH unterscheidet.
Zum Einen das direkte k-way-Splitting und zum Anderen das collapsing einer vorher konstruierten binären BVH. Wieder erfolgt der Vergleich über jede szene und jedes Splitting-verfahren. Speedup hat an dieser Stelle nicht $k = 2$, sondern die collapse-Methode als Referenz. Damit definiert sich ein neuer Vergleichswert:
\[
Speedup_{comparison} = \frac{T_{\mathrm{\mu, traversal}(k, collapse)}}{T_{\mu, \mathrm{traversal}(k, k\text{-}way)}}
\]
Wieder bezeichnet $T_{\mathrm{\mu, traversal}}$ den Mittelwert der, wie im Kapitel Methoden gezeigt, aggregierten gemessenen Traversal-Times, diesmal allerdings abhängig von der gewählten Methode zur Erzeugung der Wide BVH bei gleichem Verzweigungsgrad $k$. Wieder gitd, dass 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.2]{results/comparison_speedup_k4.png}
    \includegraphics[scale=0.2]{results/comparison_speedup_k8.png}
    \includegraphics[scale=0.2]{results/comparison_speedup_k16.png}
    \caption{Abbildung Traversal-Speedup collapse- gegen k-way- Verfahren}
    \label{fig:comparison}
\end{figure}

Die Abbildung~\ref{fig:comparison} zeigt den Traversal-Speedup für $k \in {4, 8, 16}$ und Tabelle~\ref{tab:comparison} enthält die dazugehörigen Mittelwerte, Konfidenzintervalle und p-Werte für jede kombination von Szene, Splittingverfahren und Wide BVH Methode.
Im Vergleich liegen die Speedup-Werte für alle Kombinationen Szenen und Splitting-Verfahren für alle $k \in {4, 8, 16}$ jeweils sehr nah bei $1$. Die Konfidenzintervalle reichen von $0,996$ bis $1,004$, beinhalten dabei aber immer den Wert $1$.
Dadurch lässt sich für die untersuchten statsischen Szenen kein statistisch signifikanter und damit praktisch Nutzbarer Unterschied in der Traversal-Time zwischen direktem k-way-Splitting und der collapse-methode feststellen. Dies gilt unabhängig von untersuchter Szene oder gewählten Splitting-Verfahren.

\section{Dynamisches Modell}
Die dritte und letzte Forschungsfrage untersucht die Modellierung einer dynamischen Szene, bei der die (Re-)Construction der Szene vor jeder Traversal-Phase erforderlich ist. Für die Modellierung wird eine kombinierte Qualitätsmetrik aus Construction- und Traversal-Time verwendet.
\[
T_{dynamic}(k) = T_{\mathrm{\mu, construction}(k)} + T_{\mathrm{\mu, traversal}(k)}
\]
Dabei ist $T_{\mathrm{\mu, construction}(k)}$ den Mittelwert der Construction-Time und $T_{\mathrm{\mu, traversal}(k)}$ den Mittelwert der Traversal-Time, wie im Kapitel Methoden beschrieben. Speedup wird analog zur ersten Forschungsfrage gegenüber $k = 2$ definiert:
\[
Speedup_{dynamic} = \frac{T_{\mathrm{\mu, dynamic}(k = 2)}}{T_{\mu, \mathrm{dynamic}(k)}}
\]
Ebenso entspricht $Speedup_{dynamic} > 1$ einer Verbesserung der kombinierten Metrik.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.2]{results/dynamic_combined_speedup_k4.png}
    \includegraphics[scale=0.2]{results/dynamic_combined_speedup_k8.png}
    \includegraphics[scale=0.2]{results/dynamic_combined_speedup_k16.png}
    \caption{Abbildung kombinierter Speedup für Construction- und Traversal-Time}
    \label{fig:dynamic}
\end{figure}
Die Abbildung~\ref{fig:dynamic} zeigt den Traversal-Speedup für $k \in {4, 8, 16}$ und Tabelle~\ref{tab:dynamic} enthält die dazugehörigen Ergebnisse. 
Für $k = 4$ gibt es in kleineren Szenen einen Vorteil gegenüber $k = 2$, der sich durch Speedup-Mittelwerten zwischen $1,03$ und $1,08$ ausdrückt. Für größere Szenen nimmt dieser Vorteil ab, womit der Speedup nahe an $1$ liegen, oder sogar teilweise leicht darunter fallen. 
Für $k = 8$ ist der Speedup für das dynamische Modell wieder deutlich unter $1$ mit Mittelwerten, die sich typischerweise im Bereich von $0,71$ bis $0,86$ aufhalten.
Für $k = 16$ gibt es weitere Geschwindigkeitseinbußen mit Mittelwerten für den Speedup zwischen $0,16$ und $0,30$.
Zusammengefasst zeigt das dynamische Modell ähnliche Ergebnistrends, wie im Rahmen der ersten Forschungsfrage gezeigt. Insgesamt bleibt die BVH mit $k = 4$ die Performanteste Datenstruktur für die Untersuchten Szenen, auch wenn ein Abfall für größere Szenen erkennbar ist.

\newpage

\chapter{Diskussion}

\section{Interpretation: Einfluss des Verzweigungsgrades auf die Traversal}
Vorher wurde gezeigt, dass es Einsparung in der Traversal-Time für $k = 4$ gegenüber der binären Referenz gab, während es für $k \in {8, 16}$ starke Einbußen in der Performance gab.

Eine plausible Erklärung für die beobachteten Ergebisse ist ein Trade-off zwischen der Menge an inneren Knoten und der Anzahl an auszuführenden AABB-Tests bzw. Kollisionsüberprüfungen zwischen Primärstrahl und Bounding Boxen. Bei höheren Verzweigungsgraden müssen nämlich pro Node mehr Kinder überprüft werden, während es insgesamt aber auch weniger innere Nodes gibt, die überhaupt überprüft werden können. Es lässt sich interpretieren, dass für $k = 4$ die Menge an weniger besuchten Nodes die größere menge an AABB-Tests noch überkompensiert, während dies ab $k = 8$ nicht mehr der Fall ist. Zur Veranschaulichung dient Abbildung~\ref{fig:aabb-example} in der man Beispielhaft die Menge an AABB-Tests und Triangle-Tests, bzw. Primärstrahl und Polygon Kollisionen, ablesen kann. 
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{results/aabb-test-example.png}
    \caption{AABB- und Triangle-Tests, Stanford Bunny, SAH}
    \label{fig:aabb-example}
\end{figure}
ZUdem ist wichtig, dass der für $k = 4$ beobachtete Effekt zwar statistisch signifikant ist, aber trotzdem nur einen Vorteil im einstelligen Prozenbreich liefert. Das heißt, dass Wide BVH in diesem linearen Zusammenhang eher als Optimierung mit kleinem Effekt zu sehen ist.

\section{Interpretation: K-way splitting gegen collapsing}
Im Rahmen der Ergebissvorstellung wurde gezeigt, dass die Messergebnisse unabhängig von der gewählten Methode zur Erstellung der Wide BVH ähnliche Messsungen ergaben. Die untersuchten Messergebnisse zeigten keinen signifikanten Unetrschied zwischen den Traversal-Times für die beiden Methoden.

Die naheliegenste erklärung ist, dass die räumliche Partiotionierung für beide verfahren sehr ähnlich ist. Vor allem deshalb, da beiden Methoden die gleichen Splitting-Algorithmen zu grunde liegen. Das zusammenfassen der binären BVH durch collapsing verändert zwar die Tiefe des Baumes, allerding scheint es, dass sich die Wahrscheinlichkeit für die Kollision zwischen Ray und AABB nicht sonderlich zu der durch k-way splitting erzeugten BVH unterscheidet. In Abbildung~\ref{fig:aabb-comparison} lässt sich anhand eines Beispiels ablesen, dass die Menge an AABB-Tests, positiv wie negativ, unabhängig vom gewählten Verfahren in der gleichen Größenordnung liegen. Für das Beispiel wurden wieder Stanford Bunny und SAH gewählt.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{results/aabb-test-comparison.png}
    \caption{AABB- und Triangle-Tests k-way vs. collapse, Stanford Bunny, SAH}
    \label{fig:aabb-comparison}
\end{figure}
Eine Konsequenz dieser Ergbnisse ist, dass die Wahl des Wide BVH Konstruktionsverfahren nach Implementierungsaufwand oder Construction-Time entschieden werden sollte, da es keine signifikanten einbußen durch die Wahl der untersuchten Verfahren gibt. 

\section{Interpretation: Dynamisches Modell}
Für das dynamische Modell wurde gezeigt, dass auch hier $k = 4$ überwiegend bessere Ergebnisse gegenüber der binären Referenz aufzeigt, der Effekt allerdings mit steigender Szenengröße zu sinken bzw. sich sogar leicht umzukehren scheint. Für $k = 8$ und $k = 16$ wurden klar schlechtere Ergebisse gezeigt.

Der Unterschied gegenüber der Ergebnisse der ersten Forschungsfrage ist dadurch erklärbar, dass die die Construction-Time über die verschiedenen Verzweigungsgrade hinweg für jeden Splitting-Algorithmus nahezu konstant zu sein scheint, wie man auch in Abbildung~\ref{fig:dynamic-c-vs-t} ablesen kann. Die Vorteile in der Traversal-Time werden damit durch einen konstanten offset verschlechtert, was zur Folge hat, dass der Speedup für größere Szenen gegen $1$ geht.  
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{results/dynamic-construction-vs-traversal.png}
    \caption{Vergleich Construction- gegen Traversal-Time}
    \label{fig:dynamic-c-vs-t}
\end{figure}
Vorsichtig kann man beurteilen, dass der Effekt für größere Szenen zu einem immer größer werden Problem wird, sodass der Vorteil von $k = 4$ immer weiter schwindet oder möglicherweise sogar zum Nachteil wird. Allerdings ist die untersuchte Stichprobengröße an Szenen nicht ausreichend um hierfür finale Aussagen liefern zu können, weshalb man hier von Indizien sprechn muss. 

\section{Literaturkontext}
In der Literatur werden Wide BVH eher im Kontext mit der Nutzung von SIMD oder GPUs erwähnt, da höhere Verzweigungsgrade Chancen für die bessere Parallelisierbarkeit bieten \cite{4634620}. 
Der beobachtete Einbruch des Speedups für größere $k$ ist daher mit der Literatur konsistent, da in diesem Falle der erhöhte Aufwand durch mehr AABB-Tests nicht durch parallele Berechnung armortisiert werden kann. Dadurch erscheint es so, dass ein kleinerer Verzweigungsgrad wie $ k = 4$ als Optimum für die untersuchten Szenen glaubhaft wirkt.

\section{Limitationen}
Alle besprochenen Ergebnisse gelten innerhalb der folgenden Annahmen:
\begin{itemize}
    \item Statische 3D Szenen
    \item Kamera außerhalb der Szenen
    \item Ausschließlich Untersuchung von Primärstrahlen
    \item Serielle Berechnung auf der CPU ohne spezielle Raytracing-Hardware
    \item Ausschließlich Top-Down-Konstruktion von BVH
    \item Keine echten dynamischen Verfahren, wie z.B. Refitting oder Rebuilding
\end{itemize}
Darüber hinaus sorgt die Menge an lediglich vier untersuchten Szenen, sowie ihre ähnliche Beschaffenheit für eine begrenzte Aussagekraft. Die Verteilung der untersuchten Rays ergibt sich aus der vorgegebenen Kamerafahrt um den Ursprung der Szene und nicht aus der Untersuchung von Sekundärstrahlen durch z.B. die Berechnung von Schatten. Metriken wie AABB- und Triangle-Tests wurde nur für ausgewählte Beispiele erhoben und bieten keine Grundlage für eine ausgiebige Analyse.

\section{Ausblick}
Fortlaufend bietetet es sich an die Stichprobe an Szenen zu erweitern. Mehr und vor allem größere Szenen, sowie komplexere Szenen mit Kameraursprung innerhalb der Szene biten Forschungsspielraum. Zudem wäre eine Untersuchung, die nicht nur Primärstrahlen überprüft, sondern auch Shadingeffekte, durch z.B. \emph{Ambient Occlusion}, denkbar. Auch wäre eine quantitative Untersuchung der AABB- und Triangle-Tests sinnvoll, um die hier genannten Erklärungen zu untermauern.


\newpage
\chapter{Fazit}
Der Fokus dieser Arbeit war die anaylstische Bewertung der Qualität von Top-Down konstruiierten Wide BVHs in statischen 3D Szenen aufbauend auf serieller Berechnungen. Dabei gab es das Ziel herauszufinden, ob Wide BVH alleinig auf struktureller bzw. algorithmischer Ebene einen Vorteil gegenüber binärer BVHs innehaben, auch wenn der Vorteil der parallelisierten Berechnung wegfällt.

Im Rahmen der ersten Forschungsfrage zeigten die Messungen einen statistisch signifikanten, wenn auch kleinen Vorteil durch verwendung einer BVH mit Verzweigungsgrad $k = 4$, gegenüber $k = 2$, in statischen Szenen. Für alle untersuchten Szenen und Algorithmen liegt der Speedup zwischen $1,03$ und $1,08$ für Mittelwerte. Die Qualität der BVH nimmt für $k \ge 8$ deutlich ab mit Speedup-Mittelwerten von $0,63$ bi $0,77$ für $k = 8$ und fällt für $k = 16$ sogar auf Werte zwischen $0,10$ und $0,19$. Damit lässt sich für das gewählte Szenario unter gleichen Voraussetzungn eine Empfehlung für die Nutzung von Wide BVH mit $k = 4$ aussprechen.

Die Untersuchung der zweiten Forschungsfrage hat ergeben, dass es keinen praktischen Unterschied zwischen der Nutzung der untersuchten Arten der Erzeugung von Wide BVHs in den untersuchten Szenen gibt. Über alle Konfigurationen hinweg ist der Speedup zwischen k-way splitting und collapsing ohne signifikante Abweichung nahe $1$. Damit ist die Empfehlung die gewählte Methode nach Implementierungsaufwand und Construction-Time zu wählen.

Für die Modellierung einer dynamischen Szene durch eine kombinierte Metrik aus \linebreak Construction- und Traversal-Time im Rahmen der letzten Forschungsfrage, bleiben die Ergebnisse ähnlich zur ersten Frage. Verzweigungsgrad $k = 4$ liefert insgesamt die besten Ergebnisse, während $k \ge 8$ auch in diesem Fall schlechtere Metriken liefern. Zudem lassen sich Indizien dinden, dass der Vorsprung von $k = 4$ mit setigender Szenengröße abzunehmen scheint bzw. sogar komplett ausbleiben oder sich umkehren kann. Damit kann man sagen, dass auch in diesem Zusammenhang zumindest für die kleineren Untersuchten Szenen ein Vorteil für BVHs mit Verzweigungsgrad $k = 4$ existiert.

Zusammengefasst lässt sich sagen, dass falls Wide BVHs in der seriellen Berechnung auf der CPU eingesetzt werden, man sich für einen Verzweigungsgrad von $k = 4$ entscheiden sollte, während Verzweigungsgrade $k \ge 8$ vermieden werden sollten.
Ein wichtiger nächster Schritt wäre die Messungen mit einer größeren Menge an Szenen mit höherer Polygonanzahl durchuzuführen, um die Indizien eines abnehmenden Vorteils zu untersuchen und belegbare Aussagen treffen zu können. Zudem wäre es interessant das Verhalten der BVHs unter der Nutzung von anderen Ray-Verteilungen durch beispielsweise Shader-Berechnung mit Hilfe von Sekundärestrahlen oder Ähnlichem zu beleuchten.

\newpage
\chapter{Appendix}

\begin{center}
    \input{results/static_statistical_results_table.tex}
    \input{results/comparison_statistical_results_table.tex}
    \input{results/dynamic_statistical_results_table.tex}
\end{center}

\printbibliography

\end{document}
